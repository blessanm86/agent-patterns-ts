# The Ollama model to use. Swap to qwen2.5:14b anytime for a smarter model.
MODEL=qwen2.5:7b

# Ollama host (default is fine if running locally)
OLLAMA_HOST=http://localhost:11434
